{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# about this notebook\n",
    "\n",
    "- checkout the [readme file](./readme.md) to understand how to use this notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## question\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define the user's message or query that will be sent to the models.\n",
    "- This serves as the input prompt for the language models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the user's query to be sent to the language models\n",
    "user_message = \"\"\"\n",
    "How does Photosynthesis work?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## models\n",
    "\n",
    "- **about this section**\n",
    "  - List of models that will be used for querying. Modify this list to include or exclude models as needed.\n",
    "  - Uncomment or modify the relevant lines to choose the models.\n",
    "  - For example, you can select models from Github (OpenAI, Mistral, LLaMA), Groq (Gemma & Llama), and Google (Gemini Flash & Pro).\n",
    "- **pricing**\n",
    "  - [gemini-model-list](https://ai.google.dev/gemini-api/docs/models/gemini)\n",
    "  - [gemini-pricing](https://ai.google.dev/pricing)\n",
    "  - [groq-models](https://console.groq.com/docs/models)\n",
    "  - Gemini 1.5 Flash\n",
    "    - Our fastest multimodal model with great performance for diverse, repetitive tasks and a 1 million context window.\n",
    "    - models/gemini-1.5-flash (model code)\n",
    "    - 15 RPM (requests per minute)\n",
    "    - 1 million TPM (tokens per minute)\n",
    "    - 1,500 RPD (requests per day)\n",
    "  - Gemini 1.5 Flash-8B\n",
    "    - Our smallest model for lower intelligence use cases with a 1 million token context window.\n",
    "    - models/gemini-1.5-flash-8b (model code)\n",
    "    - 15 RPM (requests per minute)\n",
    "    - 1 million TPM (tokens per minute)\n",
    "    - 1,500 RPD (requests per day)\n",
    "  - Gemini 1.5 Pro\n",
    "    - Our next-generation model with a breakthrough 2 million context window.\n",
    "    - models/gemini-1.5-pro (model code)\n",
    "    - 2 RPM (requests per minute)\n",
    "    - 32,000 TPM (tokens per minute)\n",
    "    - 50 RPD (requests per day)\n",
    "  - groq gemma 9b\n",
    "    - gemma2-9b-it (model code)\n",
    "    - 8,192 tokens (context window)\n",
    "  - groq llama 3.1 70b\n",
    "    - llama-3.1-70b-versatile (model code)\n",
    "    - 128k tokens (max_tokens limited to 8k) (context window)\n",
    "  - groq llama 3.2 3b\n",
    "    - llama-3.2-3b-preview (model code)\n",
    "    - 8k tokens (context window)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the specific model for groq or google\n",
    "groq1_model =\"gemma2-9b-it\"\n",
    "groq2_model =\"llama-3.2-3b-preview\"\n",
    "gemini1_model =\"gemini-1.5-flash-8b\"\n",
    "gemini2_model =\"gemini-1.5-pro\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the models to be used for querying\n",
    "# Uncomment the desired line or create a custom selection\n",
    "# all models\n",
    "# selected_models = [\"openai\", \"mistral\", \"llama\", \"groq1\", \"groq2\", \"gemini1\", \"gemini2\"]\n",
    "# github only\n",
    "# selected_models = [\"openai\", \"mistral\", \"llama\"]\n",
    "# groq only\n",
    "# selected_models = [\"groq1\", \"groq2\"]\n",
    "# google only\n",
    "# selected_models = [\"gemini1\", \"gemini2\"]\n",
    "# custom\n",
    "selected_models = [\"gemini1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parameters\n",
    "\n",
    "- Define the parameters for the model inference. These control the behavior of the models.\n",
    "- 'temperature' controls the randomness of the model's responses (higher values produce more random output).\n",
    "- 'top_p' is for nucleus sampling, and 'max_tokens' limits the length of the response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set inference parameters for the language models\n",
    "temperature = 1.0  # Full creativity\n",
    "top_p = 1.0  # Consider the entire probability distribution\n",
    "max_tokens = 100  # Limit the output to desired number of tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## system message\n",
    "\n",
    "- Define the system's message or instructions that will be sent to the models along with the user query.\n",
    "- The system message helps to guide the model's behavior in its response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the desired system message by uncommenting one of the following prompts\n",
    "\n",
    "# **Prompt 1: Generic Prompt**\n",
    "my_instructions = \"\"\"You are a knowledgeable and helpful assistant. Your goal is to provide accurate and helpful responses to the user's queries. Please respond accordingly.\"\"\"\n",
    "# **Prompt 2: Concise Answers**\n",
    "# my_instructions = \"\"\"You are a precise and concise assistant. Please provide brief and to-the-point answers, avoiding unnecessary explanations or elaborations. Focus on delivering the most relevant information in the fewest words possible. If the user requests more detail, be prepared to expand on your initial response.\"\"\"\n",
    "#**Prompt 3: Code-Related Questions**\n",
    "#my_instructions = \"\"\"You are an expert coding assistant. When responding to code-related queries, please: Provide accurate, well-structured, and readable code snippets.a. Ensure that your responses follow best coding practices. b. Include relevant explanations or comments to facilitate understanding. c. Suggest alternative approaches or optimizations when applicable. d. Highlight any potential issues or edge cases to consider.\"\"\"\n",
    "# **Prompt 4: Well-Thought-Out Answers**\n",
    "# my_instructions = \"\"\"You are a meticulous and thorough assistant. Before providing a response: a. Take the time to thoroughly review and validate your answer. b. Ensure that it is accurate, comprehensive, and relevant to the user's query. c. Consider multiple perspectives or approaches to the problem. d. Prioritize the quality of your response over speed. e. Do not hesitate to request clarification if needed. f. Provide a structured response with clear reasoning for your conclusions.\"\"\"\n",
    "# **Prompt 5: Simple Explanations with Examples**\n",
    "# my_instructions = \"\"\"You are a patient and explanatory assistant. When responding to user queries: a. Provide clear and concise explanations. b. Use relatable examples to illustrate concepts. c. Avoid technical jargon or complex terminology. d. Focus on making the explanation accessible and understandable to a layperson. e. Use analogies or metaphors to help clarify difficult concepts. f. If needed, break down complex ideas into simpler components.\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries and modules\n",
    "import os\n",
    "from typing import Dict, Any\n",
    "from openai import OpenAI\n",
    "from mistralai import Mistral, UserMessage as MistralUserMessage, SystemMessage as MistralSystemMessage\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import SystemMessage as AzureSystemMessage, UserMessage as AzureUserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up API endpoints and authentication tokens\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "github_token = os.environ[\"GITHUB_TOKEN\"]\n",
    "groq_token = os.environ[\"GROQ_TOKEN\"]\n",
    "gemini_token = os.environ[\"GEMINI_TOKEN\"]\n",
    "genai.configure(api_key=gemini_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# logic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## github models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### github openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/marketplace/models/azure-openai/gpt-4o-mini\n",
    "\n",
    "def get_openai_response(instructions: str, user_message: str, model_params: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Send a request to the OpenAI API and get a response.\n",
    "\n",
    "    Args:\n",
    "    instructions (str): System message to guide the model's behavior\n",
    "    user_message (str): User's query or message\n",
    "    model_params (Dict[str, Any]): Parameters for the API call\n",
    "\n",
    "    Returns:\n",
    "    str: The model's response\n",
    "    \"\"\"\n",
    "    # Initialize the OpenAI client with the provided endpoint and API key\n",
    "    client = OpenAI(\n",
    "        base_url=model_params['endpoint'],\n",
    "        api_key=model_params['token'],\n",
    "    )\n",
    "\n",
    "    # Send a chat completion request to the OpenAI API\n",
    "    response = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": instructions},\n",
    "            {\"role\": \"user\", \"content\": user_message},\n",
    "        ],\n",
    "        temperature=model_params['temperature'],\n",
    "        top_p=model_params['top_p'],\n",
    "        max_tokens=model_params['max_tokens'],\n",
    "        model=model_params['model_name']\n",
    "    )\n",
    "\n",
    "    # Return the content of the first choice in the API response\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### github mistral\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/marketplace/models/azureml-mistral/Mistral-large-2407\n",
    "def get_mistral_response(instructions: str, user_message: str, model_params: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Send a request to the Mistral API and get a response.\n",
    "\n",
    "    Args:\n",
    "    instructions (str): System message to guide the model's behavior\n",
    "    user_message (str): User's query or message\n",
    "    model_params (Dict[str, Any]): Parameters for the API call\n",
    "\n",
    "    Returns:\n",
    "    str: The model's response\n",
    "    \"\"\"\n",
    "    # Initialize the Mistral client with the provided API key and server URL\n",
    "    client = Mistral(api_key=model_params['token'], server_url=model_params['endpoint'])\n",
    "\n",
    "    # Send a chat completion request to the Mistral API\n",
    "    response = client.chat.complete(\n",
    "        model=model_params['model_name'],\n",
    "        messages=[\n",
    "            MistralSystemMessage(content=instructions),\n",
    "            MistralUserMessage(content=user_message),\n",
    "        ],\n",
    "        temperature=model_params['temperature'],\n",
    "        top_p=model_params['top_p'],\n",
    "        max_tokens=model_params['max_tokens'],\n",
    "    )\n",
    "\n",
    "    # Return the content of the first choice in the API response\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### github llama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/marketplace/models/azureml-meta/Meta-Llama-3-1-405B-Instruct\n",
    "def get_llama_response(instructions: str, user_message: str, model_params: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Send a request to the LLaMA API and get a response.\n",
    "\n",
    "    Args:\n",
    "    instructions (str): System message to guide the model's behavior\n",
    "    user_message (str): User's query or message\n",
    "    model_params (Dict[str, Any]): Parameters for the API call\n",
    "\n",
    "    Returns:\n",
    "    str: The model's response\n",
    "    \"\"\"\n",
    "    # Initialize the ChatCompletionsClient with the provided endpoint and API key\n",
    "    client = ChatCompletionsClient(\n",
    "        endpoint=model_params['endpoint'],\n",
    "        credential=AzureKeyCredential(model_params['token']),\n",
    "    )\n",
    "\n",
    "    # Send a chat completion request to the LLaMA API\n",
    "    response = client.complete(\n",
    "        messages=[\n",
    "            AzureSystemMessage(content=instructions),\n",
    "            AzureUserMessage(content=user_message),\n",
    "        ],\n",
    "        temperature=model_params['temperature'],\n",
    "        top_p=model_params['top_p'],\n",
    "        max_tokens=model_params['max_tokens'],\n",
    "        model=model_params['model_name']\n",
    "    )\n",
    "\n",
    "    # Return the content of the first choice in the API response\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## groq models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://console.groq.com/docs/models\n",
    "def get_groq_response(instructions: str, user_message: str, model_params: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Send a request to the Groq API and get a response.\n",
    "\n",
    "    Args:\n",
    "    instructions (str): System message to guide the model's behavior\n",
    "    user_message (str): User's query or message\n",
    "    model_params (Dict[str, Any]): Parameters for the API call\n",
    "\n",
    "    Returns:\n",
    "    str: The model's response\n",
    "    \"\"\"\n",
    "    from groq import Groq\n",
    "\n",
    "    # Initialize the Groq client with the provided API key\n",
    "    client = Groq(api_key=model_params['token'])\n",
    "\n",
    "    # Send a chat completion request to the Groq API\n",
    "    response = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": instructions},\n",
    "            {\"role\": \"user\", \"content\": user_message},\n",
    "        ],\n",
    "        model=model_params['model_name'],\n",
    "        temperature=model_params['temperature'],\n",
    "        top_p=model_params['top_p'],\n",
    "        max_tokens=model_params['max_tokens'],\n",
    "    )\n",
    "\n",
    "    # Return the content of the first choice in the API response\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gemini models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://ai.google.dev/gemini-api/docs/models/gemini\n",
    "def get_gemini_response(instructions: str, user_message: str, model_params: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Send a request to the Gemini API and get a response.\n",
    "\n",
    "    Args:\n",
    "    instructions (str): System message to guide the model's behavior\n",
    "    user_message (str): User's query or message\n",
    "    model_params (Dict[str, Any]): Parameters for the API call\n",
    "\n",
    "    Returns:\n",
    "    str: The model's response\n",
    "    \"\"\"\n",
    "    client = genai.GenerativeModel(model_params[\"model_name\"])\n",
    "\n",
    "    response = client.generate_content(f\"{instructions}\\n\\n{user_message}\")\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model-specific parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters for each model, including model name, endpoint, token, and inference settings\n",
    "model_params = {\n",
    "\"openai\": {\n",
    "\"model_name\": \"gpt-4o\",\n",
    "\"endpoint\": endpoint,\n",
    "\"token\": github_token,\n",
    "\"temperature\": temperature,\n",
    "\"top_p\": top_p,\n",
    "\"max_tokens\": max_tokens,\n",
    "},\n",
    "\"mistral\": {\n",
    "\"model_name\": \"Mistral-large-2407\",\n",
    "\"endpoint\": endpoint,\n",
    "\"token\": github_token,\n",
    "\"temperature\": temperature,\n",
    "\"top_p\": top_p,\n",
    "\"max_tokens\": max_tokens,\n",
    "},\n",
    "\"llama\": {\n",
    "\"model_name\": \"meta-llama-3.1-405b-instruct\",\n",
    "\"endpoint\": endpoint,\n",
    "\"token\": github_token,\n",
    "\"temperature\": temperature,\n",
    "\"top_p\": top_p,\n",
    "\"max_tokens\": max_tokens,\n",
    "},\n",
    "\"groq1\": {\n",
    "\t\t\"model_name\": groq1_model,\n",
    "\t\t\"endpoint\": endpoint,\n",
    "\t\t\"token\": groq_token,\n",
    "\t\t\"temperature\": temperature,\n",
    "\t\t\"top_p\": top_p,\n",
    "\t\t\"max_tokens\": max_tokens,\n",
    "},\n",
    "\"groq2\": {\n",
    "\t\t\"model_name\": groq2_model,\n",
    "\t\t\"endpoint\": endpoint,\n",
    "\t\t\"token\": groq_token,\n",
    "\t\t\"temperature\": temperature,\n",
    "\t\t\"top_p\": top_p,\n",
    "\t\t\"max_tokens\": max_tokens,\n",
    "},\n",
    "# https://ai.google.dev/api/generate-content#v1beta.GenerationConfig\n",
    "\"gemini1\": {\n",
    "\t\t\"model_name\": gemini1_model,\n",
    "\t\t\"token\": gemini_token,\n",
    "\t\t\"temperature\": temperature,\n",
    "\t\t\"top_p\": top_p,\n",
    "\t\t\"max_tokens\": max_tokens,\n",
    "},\n",
    "\"gemini2\": {\n",
    "\t\t\"model_name\": gemini2_model,\n",
    "\t\t\"token\": gemini_token,\n",
    "\t\t\"temperature\": temperature,\n",
    "\t\t\"top_p\": top_p,\n",
    "\t\t\"max_tokens\": max_tokens,\n",
    "},\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the appropriate function for each selected model to get responses\n",
    "openai_response = get_openai_response(my_instructions, user_message, model_params[\"openai\"])\n",
    "mistral_response = get_mistral_response(my_instructions, user_message, model_params[\"mistral\"])\n",
    "llama_response = get_llama_response(my_instructions, user_message, model_params[\"llama\"])\n",
    "groq1_response = get_groq_response(my_instructions, user_message, model_params[\"groq1\"])\n",
    "groq2_response = get_groq_response(my_instructions, user_message, model_params[\"groq2\"])\n",
    "gemini1_response = get_gemini_response(my_instructions, user_message, model_params[\"gemini1\"])\n",
    "gemini2_response = get_gemini_response(my_instructions, user_message, model_params[\"gemini2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from datetime import datetime\n",
    "from typing import Callable, Dict\n",
    "\n",
    "class DelimiterOutput:\n",
    "    \"\"\"\n",
    "    A context manager class to redirect stdout to a file and add delimiters.\n",
    "    \"\"\"\n",
    "    def __init__(self, file):\n",
    "        self.file = file\n",
    "        self.original_stdout = sys.stdout\n",
    "\n",
    "    def __enter__(self):\n",
    "        sys.stdout = self\n",
    "        return self\n",
    "\n",
    "    def write(self, text):\n",
    "        self.file.write(text)\n",
    "\n",
    "    def flush(self):\n",
    "        pass\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        sys.stdout = self.original_stdout\n",
    "        self.file.write(\"\\n\")\n",
    "\n",
    "class ResponsePrinter:\n",
    "    \"\"\"\n",
    "    A context manager class to handle file operations for printing responses.\n",
    "    \"\"\"\n",
    "    def __init__(self, file_name: str):\n",
    "        self.file_name = file_name\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.file = open(self.file_name, 'r+')\n",
    "        self.content = self.file.read()\n",
    "        self.file.seek(0)\n",
    "        self.file.write(\"\")\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        self.file.write(self.content)\n",
    "        self.file.close()\n",
    "\n",
    "def print_response(file_name: str, header: str, response: str, model_name: str):\n",
    "    \"\"\"\n",
    "    Print a response with a header to a file.\n",
    "\n",
    "    Args:\n",
    "    file_name (str): Name of the file to write to\n",
    "    header (str): Header for the response\n",
    "    response (str): The response content\n",
    "    model_name (str): Name of the model that generated the response\n",
    "    \"\"\"\n",
    "    with ResponsePrinter(file_name) as printer, DelimiterOutput(printer.file):\n",
    "        header_with_model = f\"## {header} ({model_name})\" if model_name else f\"## {header}\"\n",
    "        print(f\"{header_with_model}\\n\\n{response}\")\n",
    "\n",
    "def print_user_message(file_name: str, response: str, _: str):\n",
    "    \"\"\"\n",
    "    Print the user's message with a timestamp to a file.\n",
    "\n",
    "    Args:\n",
    "    file_name (str): Name of the file to write to\n",
    "    response (str): The user's message\n",
    "    _ (str): Unused parameter (for consistency with other print functions)\n",
    "    \"\"\"\n",
    "    with ResponsePrinter(file_name) as printer, DelimiterOutput(printer.file):\n",
    "        current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        print(f\"# Question {current_time}\\n{response}\")\n",
    "\n",
    "# Define a dictionary of response printers for each model and special sections\n",
    "response_printers: Dict[str, Callable[[str, str, str], None]] = {\n",
    "    \"gemini2\": lambda file, response, model: print_response(file, \"Gemini 2\", response, model),\n",
    "    \"gemini1\": lambda file, response, model: print_response(file, \"Gemini 1\", response, model),\n",
    "    \"groq2\": lambda file, response, model: print_response(file, \"Groq 2\", response, model),\n",
    "    \"groq1\": lambda file, response, model: print_response(file, \"Groq 1\", response, model),\n",
    "    \"llama\": lambda file, response, model: print_response(file, \"Llama\", response, model),\n",
    "    \"mistral\": lambda file, response, model: print_response(file, \"Mistral\", response, model),\n",
    "    \"openai\": lambda file, response, model: print_response(file, \"OpenAI\", response, model),\n",
    "    \"instructions\": lambda file, response, _: print_response(file, \"My Instructions\", response, \"\"),\n",
    "    \"user_message\": print_user_message\n",
    "}\n",
    "\n",
    "# Define a function that prints the responses from all the models in a specific order.\n",
    "# The function takes in a file name, a dictionary of responses, and the parameters of each model.\n",
    "# It prints out the responses to a file in a user-friendly format.\n",
    "def print_all_responses(file_name: str, responses: Dict[str, str], model_params: Dict[str, Dict[str, Any]]):\n",
    "    \"\"\"\n",
    "    Print all responses from different models to a file in a specific order.\n",
    "\n",
    "    Args:\n",
    "    file_name (str): Name of the file to write the responses to\n",
    "    responses (Dict[str, str]): Dictionary containing responses from different models\n",
    "    model_params (Dict[str, Dict[str, Any]]): Dictionary containing parameters for each model\n",
    "    \"\"\"\n",
    "    # Define the desired order in which the model responses should be printed\n",
    "    section_order = ['gemini2', 'gemini1', 'groq2', 'groq1', 'llama', 'mistral', 'openai', 'instructions', 'user_message']\n",
    "\n",
    "    # Iterate over each section in the defined order\n",
    "    for key in section_order:\n",
    "        if key in responses:\n",
    "            # If the key exists in the responses and a printer function is defined for it,\n",
    "            # retrieve the model's name (if available) and print the response.\n",
    "            if key in response_printers:\n",
    "                model_name = model_params[key][\"model_name\"] if key in model_params else \"\"\n",
    "                response_printers[key](file_name, responses[key], model_name)\n",
    "\n",
    "# After gathering all responses, this block handles calling the API for each selected model and storing the results in the 'responses' dictionary. The responses are then printed using the defined print_all_responses function.\n",
    "responses = {}\n",
    "\n",
    "# Iterate over the selected models (defined earlier as a list of model names).\n",
    "# This block determines which backend to call for each selected model.\n",
    "# Depending on the model, the respective API call function is used to retrieve a response.\n",
    "for model in selected_models:\n",
    "    if model == \"openai\":\n",
    "        responses[\"openai\"] = get_openai_response(my_instructions, user_message, model_params[\"openai\"])\n",
    "    elif model == \"mistral\":\n",
    "        responses[\"mistral\"] = get_mistral_response(my_instructions, user_message, model_params[\"mistral\"])\n",
    "    elif model == \"llama\":\n",
    "        responses[\"llama\"] = get_llama_response(my_instructions, user_message, model_params[\"llama\"])\n",
    "    elif model == \"groq1\":\n",
    "        responses[\"groq1\"] = get_groq_response(my_instructions, user_message, model_params[\"groq1\"])\n",
    "    elif model == \"groq2\":\n",
    "        responses[\"groq2\"] = get_groq_response(my_instructions, user_message, model_params[\"groq2\"])\n",
    "    elif model == \"gemini1\":\n",
    "        responses[\"gemini1\"] = get_gemini_response(my_instructions, user_message, model_params[\"gemini1\"])\n",
    "    elif model == \"gemini2\":\n",
    "        responses[\"gemini2\"] = get_gemini_response(my_instructions, user_message, model_params[\"gemini2\"])\n",
    "\n",
    "# Once all models have been queried, store the original instructions and user message in the responses dictionary for later use or reference in the output.\n",
    "responses[\"instructions\"] = my_instructions\n",
    "responses[\"user_message\"] = user_message\n",
    "\n",
    "# Print all responses to the 'response.md' file\n",
    "print_all_responses('response.md', responses, model_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "playground_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
